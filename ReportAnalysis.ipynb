{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Blob Inventory Analytics**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the details of account and container where configuration.json file is stored\r\n",
        "# and that storage account is connected to synapse workspace\r\n",
        "storage_account = \"priyashreehns1611\"\r\n",
        "container_name = \"links\"\r\n",
        "file_name = \"configuration\"\r\n",
        "\r\n",
        "# name of the database in which tables will be stored\r\n",
        "database_name = \"temp1\""
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 1,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:06.5706223Z",
              "session_start_time": "2022-07-02T13:15:06.6229227Z",
              "execution_start_time": "2022-07-02T13:19:27.7103178Z",
              "execution_finish_time": "2022-07-02T13:19:27.8804535Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 1, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall azure-storage-blob --yes\r\n",
        "!pip install azure-storage-blob==2.1.0"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 2,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:06.6666112Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:27.9824132Z",
              "execution_finish_time": "2022-07-02T13:19:46.7790685Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 2, Finished, Available)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: azure-storage-blob 12.8.1\nUninstalling azure-storage-blob-12.8.1:\n  Successfully uninstalled azure-storage-blob-12.8.1\nCollecting azure-storage-blob==2.1.0\n  Downloading azure_storage_blob-2.1.0-py2.py3-none-any.whl (88 kB)\n\u001b[K     |████████████████████████████████| 88 kB 14.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: azure-common>=1.1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from azure-storage-blob==2.1.0) (1.1.27)\nCollecting azure-storage-common~=2.1\n  Downloading azure_storage_common-2.1.0-py2.py3-none-any.whl (47 kB)\n\u001b[K     |████████████████████████████████| 47 kB 16.6 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.25.1)\nRequirement already satisfied: python-dateutil in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.8.1)\nRequirement already satisfied: cryptography in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from azure-storage-common~=2.1->azure-storage-blob==2.1.0) (3.4.7)\nRequirement already satisfied: cffi>=1.12 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from cryptography->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.14.5)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from cffi>=1.12->cryptography->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.20)\nRequirement already satisfied: six>=1.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from python-dateutil->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.16.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2021.5.30)\nRequirement already satisfied: idna<3,>=2.5 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (2.10)\nRequirement already satisfied: chardet<5,>=3.0.2 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (4.0.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /home/trusted-service-user/cluster-env/env/lib/python3.8/site-packages (from requests->azure-storage-common~=2.1->azure-storage-blob==2.1.0) (1.26.4)\nInstalling collected packages: azure-storage-common, azure-storage-blob\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nadlfs 0.7.7 requires azure-storage-blob>=12.5.0, but you have azure-storage-blob 2.1.0 which is incompatible.\u001b[0m\nSuccessfully installed azure-storage-blob-2.1.0 azure-storage-common-2.1.0\n"
          ]
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\r\n",
        "from pyspark.sql import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.functions import count as count\r\n",
        "from pyspark.sql.functions import substring_index\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from azure.storage.blob import BlockBlobService\r\n",
        "import datetime"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 3,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:06.8120268Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:46.9256526Z",
              "execution_finish_time": "2022-07-02T13:19:49.6794885Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 3, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing distribution of data in different containers \r\n",
        "def store_data_distribution_in_containers_details(csvFile, database_name):\r\n",
        "    container_info_df = csvFile.withColumn(\"ContainerName\", substring_index(csvFile.Name, '/', 1))\r\n",
        "    container_info_df = container_info_df.groupBy(['ReportGenerationDate','ContainerName']).sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    container_info_df.write.mode('ignore').saveAsTable(\"{0}.ContainerInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 4,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.06983Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:49.8341033Z",
              "execution_finish_time": "2022-07-02T13:19:49.9725188Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 4, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_distribution_in_blob_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing BlobType and their corresponding count\r\n",
        "    blob_type_info_df = csvFile.groupBy(['ReportGenerationDate','BlobType']).agg(count('BlobType').alias('BlobTypeCount'))\r\n",
        "    blob_type_info_df.write.mode('ignore').saveAsTable(\"{0}.BlobTypeInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 5,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.1904028Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:50.0835913Z",
              "execution_finish_time": "2022-07-02T13:19:50.2232101Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 5, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_distribution_in_access_tier_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing AccessTier and its corresponding count\r\n",
        "    access_tier_info_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\").groupBy(['ReportGenerationDate','AccessTier']).agg(count('AccessTier').alias('AccessTierCount'))\r\n",
        "    access_tier_info_df.write.mode('ignore').saveAsTable(\"{0}.AccessTierInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 6,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.3001735Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:50.3354004Z",
              "execution_finish_time": "2022-07-02T13:19:50.4777942Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 6, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_soft_deleted_data_size_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing soft deleted data size\r\n",
        "    if \"Deleted\" in csvFile.columns:\r\n",
        "        soft_deleted_info_df = csvFile.filter(csvFile.Deleted==True).groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    else:\r\n",
        "        soft_deleted_info_df = spark.createDataFrame([Row(\"9999-12-31T00:00:00Z\",0)],[\"ReportGenerationDate\",\"Size\"])\r\n",
        "    if len(soft_deleted_info_df.head(1))==0:\r\n",
        "        soft_deleted_info_df = spark.createDataFrame([Row(\"9999-12-31T00:00:00Z\",0)],[\"ReportGenerationDate\",\"Size\"])\r\n",
        "    soft_deleted_info_df.write.mode('ignore').saveAsTable(\"{0}.SoftDeletedInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 7,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.4230323Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:50.6079252Z",
              "execution_finish_time": "2022-07-02T13:19:50.763841Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 7, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_content_type_distribution_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing File type and its corresponding size\r\n",
        "    if \"hdi_isfolder\" in csvFile.columns:\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['hdi_isfolder']==True,'Unknown').otherwise(substring_index(csvFile['Name'], '.', -1)))\r\n",
        "    else:\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['Name'].contains(\".\"),substring_index(csvFile['Name'], '.', -1)).otherwise('Unknown'))\r\n",
        "    content_type_info_by_count_df = content_type_info_df.groupBy(['ReportGenerationDate','FileType']).agg(sum('Content-Length').alias('Sum'))\r\n",
        "    content_type_info_by_count_df.write.mode('ignore').saveAsTable(\"{0}.ContentTypeInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 8,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.5745361Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:50.8645477Z",
              "execution_finish_time": "2022-07-02T13:19:51.0034777Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 8, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_growth_in_account_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing Date and corresponding data size on that day \r\n",
        "    growth_in_data_df = csvFile.groupBy('ReportGenerationDate').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\").orderBy(\"ReportGenerationDate\",\"Size\")\r\n",
        "    growth_in_data_df.write.mode('ignore').saveAsTable(\"{0}.dailyDataSizeInfo\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 9,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.6931653Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:51.1856234Z",
              "execution_finish_time": "2022-07-02T13:19:51.3326744Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 9, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_creation_in_account_details(csvFile, database_name):\r\n",
        "    data_creation_with_time_df = csvFile.withColumn(\"Date\", to_date(csvFile['Creation-Time'], 'dd-MM-yyyy'))\r\n",
        "    data_creation_with_time_df = data_creation_with_time_df.groupBy('Date').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    data_creation_with_time_df.write.mode('ignore').saveAsTable(\"{0}.dataCreationWithTime\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 10,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.8045228Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:51.4441781Z",
              "execution_finish_time": "2022-07-02T13:19:51.6224729Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 10, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_last_access_time_details(csvFile, database_name):\r\n",
        "    last_access_time_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\")\r\n",
        "    last_access_time_df = last_access_time_df.filter(last_access_time_df.AccessTier==\"Hot\")\r\n",
        "    if \"LastAccessTime\" in csvFile.columns and len(last_access_time_df.head(1))!=0:\r\n",
        "        max_ts = last_access_time_df.agg({\"ReportGenerationDate\": \"max\"}).collect()[0][0]\r\n",
        "        min_ts = last_access_time_df.agg({\"ReportGenerationDate\": \"min\"}).collect()[0][0]\r\n",
        "        last_access_time_df = last_access_time_df.filter(last_access_time_df.ReportGenerationDate==max_ts)\r\n",
        "        last_access_time_df = last_access_time_df.withColumn(\"DaysLastAccessed\", when((last_access_time_df[\"LastAccessTime\"].isNull() | (last_access_time_df[\"LastAccessTime\"]=='')),(datetime.datetime.now().date()-min_ts).days).otherwise(((unix_timestamp(current_date(),\"dd\") - unix_timestamp(last_access_time_df[\"LastAccessTime\"], \"dd\"))/86400).cast(IntegerType())))\r\n",
        "        last_access_time_df = last_access_time_df.withColumn(\"TotalSize\",lit(last_access_time_df.agg({\"Content-Length\":\"sum\"}).collect()[0][0]))\r\n",
        "        last_access_time_df.select(\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\").write.mode('ignore').saveAsTable(\"{0}.lastAccessTime\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 11,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:07.9168962Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:51.7341863Z",
              "execution_finish_time": "2022-07-02T13:19:51.8837723Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 11, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_occupied_by_snapshot_details(csvFile, database_name):\r\n",
        "    if \"Snapshot\" in csvFile.columns:\r\n",
        "        snapshot_data_df = csvFile.na.drop(how='all',subset=['Snapshot'])\r\n",
        "        snapshot_data_df = snapshot_data_df.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "        snapshot_data_df.write.mode('ignore').saveAsTable(\"{0}.snapshotData\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 12,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.0236726Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:51.9803813Z",
              "execution_finish_time": "2022-07-02T13:19:52.1440949Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 12, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_modifications_in_data_details(csvFile, database_name):\r\n",
        "    last_modified_count_df = csvFile.withColumn(\"Date\", to_date(csvFile['Last-Modified'], 'dd-MM-yyyy'))\r\n",
        "    last_modified_count_df = last_modified_count_df.groupBy('Date').agg(count('Date').alias('NumberOfModifications'))\r\n",
        "    last_modified_count_df.write.mode('ignore').saveAsTable(\"{0}.lastModifiedCount\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 13,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.1369128Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:52.2728005Z",
              "execution_finish_time": "2022-07-02T13:19:52.4629777Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 13, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_reports_analysed_dates(csvFile, database_name):\r\n",
        "    reports_analysed_df = csvFile.select(\"ReportGenerationDate\").distinct()\r\n",
        "    reports_analysed_df.write.mode('ignore').saveAsTable(\"{0}.reportsanalysed\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 14,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.2469085Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:52.6073473Z",
              "execution_finish_time": "2022-07-02T13:19:52.7681513Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 14, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 26,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name):\r\n",
        "    blob_service = BlockBlobService(storage_account_name, access_key)\r\n",
        "    blob_list = blob_service.list_blobs(destination_container)\r\n",
        "    links_list = []\r\n",
        "    for blob in blob_list:\r\n",
        "        if rule_name+\"-manifest.json\" in blob.name:\r\n",
        "            link = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container,storage_account_name,blob.name)\r\n",
        "            links_list.append(link)\r\n",
        "    return links_list"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 15,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.4069934Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:52.9049963Z",
              "execution_finish_time": "2022-07-02T13:19:53.0610683Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 15, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 27,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populating_tables(csvFile, database_name):\r\n",
        "    store_data_distribution_in_containers_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_blob_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_access_tier_details(csvFile, database_name)\r\n",
        "    store_soft_deleted_data_size_details(csvFile, database_name)\r\n",
        "    store_content_type_distribution_details(csvFile, database_name)\r\n",
        "    store_data_growth_in_account_details(csvFile, database_name)\r\n",
        "    store_data_creation_in_account_details(csvFile, database_name)\r\n",
        "    store_last_access_time_details(csvFile, database_name)\r\n",
        "    store_data_occupied_by_snapshot_details(csvFile, database_name)\r\n",
        "    store_modifications_in_data_details(csvFile, database_name)\r\n",
        "    store_reports_analysed_dates(csvFile, database_name)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 16,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.5274388Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:53.1868166Z",
              "execution_finish_time": "2022-07-02T13:19:53.334967Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 16, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setting_credentials(storage_account_name, access_key):\r\n",
        "    access_link = \"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name)\r\n",
        "    spark.conf.set(access_link,access_key)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 17,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.6236726Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:53.4654441Z",
              "execution_finish_time": "2022-07-02T13:19:53.6188801Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 17, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 29,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processing_reports(list_of_report_links_json_file):\r\n",
        "    report_df = spark.read.option(\"multiLine\", \"true\").json(list_of_report_links_json_file[0])\r\n",
        "    reports_analysed = []\r\n",
        "    data_collect = report_df.collect()[0]\r\n",
        "\r\n",
        "    # initialising dataframe with first row\r\n",
        "    destination_container = data_collect[\"destinationContainer\"]\r\n",
        "    prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "    report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "    reports_analysed.append(report_generation_date)\r\n",
        "    \r\n",
        "    file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "    fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "    \r\n",
        "    if file_format==\"csv\":\r\n",
        "        fileData = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "    else:\r\n",
        "        fileData = spark.read.parquet(fileLink)\r\n",
        "    \r\n",
        "    # appending inventory report generation date to the dataframe\r\n",
        "    fileData = fileData.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "\r\n",
        "    # iterating over rest of the the rows\r\n",
        "    for json_file_report_link in list_of_report_links_json_file[1:]:\r\n",
        "\r\n",
        "        report_df = spark.read.option(\"multiLine\", \"true\").json(json_file_report_link)\r\n",
        "        data_collect = report_df.collect()[0]\r\n",
        "\r\n",
        "        if(data_collect[\"status\"]=='Pending'):\r\n",
        "            continue\r\n",
        "        \r\n",
        "        destination_container = data_collect[\"destinationContainer\"]\r\n",
        "        prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "        report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "\r\n",
        "        if report_generation_date in reports_analysed:\r\n",
        "            continue\r\n",
        "\r\n",
        "        file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "        fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "\r\n",
        "        if file_format==\"csv\":\r\n",
        "            fileDataTemp = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "        else:\r\n",
        "            fileDataTemp = spark.read.parquet(fileLink)\r\n",
        "\r\n",
        "        fileDataTemp = fileDataTemp.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "        reports_analysed.append(report_generation_date)\r\n",
        "        fileData = fileData.unionByName(fileDataTemp, allowMissingColumns=True)\r\n",
        "\r\n",
        "    # changing datatype of column Content-Length from string to integer\r\n",
        "    fileData = fileData.withColumn(\"Content-Length\", fileData[\"Content-Length\"].cast(IntegerType()))\r\n",
        "    # changing datatype of column ReportGenerationDate to Date type\r\n",
        "    fileData = fileData.withColumn(\"ReportGenerationDate\", to_date('ReportGenerationDate'))\r\n",
        "    # changing datatype of column LastAccessTime to Date type\r\n",
        "    fileData = fileData.withColumn(\"LastAccessTime\", to_date('LastAccessTime'))\r\n",
        "    return fileData"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 18,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.7591913Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:53.732687Z",
              "execution_finish_time": "2022-07-02T13:19:53.8859212Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 18, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 30,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_template_data(storage_account, container_name, file_name):\r\n",
        "    json_file_link = \"abfss://{0}@{1}.dfs.core.windows.net/{2}.json\".format(container_name, storage_account, file_name)\r\n",
        "    json_file = spark.read.option(\"multiLine\", \"true\").json(json_file_link)\r\n",
        "    json_file_data = json_file.collect()[0]\r\n",
        "    return json_file_data"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 19,
              "state": "finished",
              "livy_statement_state": "available",
              "queued_time": "2022-07-02T13:15:08.8703619Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:54.0231143Z",
              "execution_finish_time": "2022-07-02T13:19:54.1559722Z"
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 19, Finished, Available)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data from template file\r\n",
        "json_file_data = get_template_data(storage_account, container_name, file_name)\r\n",
        "\r\n",
        "# initializing with the data stored in template file\r\n",
        "storage_account_name = json_file_data[\"storageAccountName\"]\r\n",
        "access_key = json_file_data[\"accessKey\"]\r\n",
        "destination_container = json_file_data[\"destinationContainer\"]\r\n",
        "rule_name = json_file_data[\"ruleName\"]\r\n",
        "\r\n",
        "# setting credentials for the spark session\r\n",
        "setting_credentials(storage_account_name, access_key)\r\n",
        "\r\n",
        "# list of all the report links\r\n",
        "list_of_report_links = get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name)\r\n",
        "# print(\"No of reports - \",len(list_of_report_links))\r\n",
        "\r\n",
        "# processsing all the reports in a dataframe\r\n",
        "file_data = processing_reports(list_of_report_links)\r\n",
        "\r\n",
        "# creating database if it does not exist\r\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS {0}\".format(database_name))\r\n",
        "\r\n",
        "# storing all processed dataframes in respective tables\r\n",
        "populating_tables(file_data, database_name)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": "sparktemp1",
              "session_id": 18,
              "statement_id": 20,
              "state": "submitted",
              "livy_statement_state": "running",
              "queued_time": "2022-07-02T13:15:08.9544356Z",
              "session_start_time": null,
              "execution_start_time": "2022-07-02T13:19:54.2910708Z",
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(sparktemp1, 18, 20, Submitted, Running)"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No of reports -  6\n"
          ]
        }
      ],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# command to drop the database and the corresponding tables\r\n",
        "# database_name = \"temp1\"\r\n",
        "# spark.sql(\"DROP DATABASE IF EXISTS {0} CASCADE\".format(database_name))"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.livy.statement-meta+json": {
              "spark_pool": null,
              "session_id": null,
              "statement_id": null,
              "state": "waiting",
              "livy_statement_state": null,
              "queued_time": "2022-07-02T13:15:09.0290894Z",
              "session_start_time": null,
              "execution_start_time": null,
              "execution_finish_time": null
            },
            "text/plain": "StatementMeta(, , , Waiting, )"
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}