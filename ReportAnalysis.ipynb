{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Blob Inventory Analytics**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the details of account and container where configuration.json file is stored\r\n",
        "# and that storage account is connected to synapse workspace\r\n",
        "storage_account = \"reportanalysis\"\r\n",
        "container_name = \"reportdata\"\r\n",
        "file_name = \"configuration\"\r\n",
        "\r\n",
        "# name of the database in which tables will be stored\r\n",
        "database_name = \"reportdata\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall azure-storage-blob --yes\r\n",
        "!pip install azure-storage-blob==2.1.0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\r\n",
        "from pyspark.sql import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.functions import count as count\r\n",
        "from pyspark.sql.functions import substring_index\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from azure.storage.blob import BlockBlobService\r\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing distribution of data in different containers \r\n",
        "def store_data_distribution_in_containers_details(csvFile, database_name):\r\n",
        "    container_info_df = csvFile.withColumn(\"ContainerName\", substring_index(csvFile.Name, '/', 1))\r\n",
        "    container_info_df = container_info_df.groupBy(['ReportGenerationDate','ContainerName']).sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    container_info_df.write.mode('ignore').saveAsTable(\"{0}.ContainerInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_distribution_in_blob_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing BlobType and their corresponding count\r\n",
        "    blob_type_info_df = csvFile.groupBy(['ReportGenerationDate','BlobType']).agg(count('BlobType').alias('BlobTypeCount'))\r\n",
        "    blob_type_info_df.write.mode('ignore').saveAsTable(\"{0}.BlobTypeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_distribution_in_access_tier_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing AccessTier and its corresponding count\r\n",
        "    access_tier_info_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\").groupBy(['ReportGenerationDate','AccessTier']).agg(count('AccessTier').alias('AccessTierCount'))\r\n",
        "    access_tier_info_df.write.mode('ignore').saveAsTable(\"{0}.AccessTierInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_soft_deleted_data_size_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing soft deleted data size\r\n",
        "    if \"Deleted\" in csvFile.columns:\r\n",
        "        soft_deleted_info_df = csvFile.filter(csvFile.Deleted==True).groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    else:\r\n",
        "        soft_deleted_info_df = spark.createDataFrame([Row(\"9999-12-31T00:00:00Z\",0)],[\"ReportGenerationDate\",\"Size\"])\r\n",
        "    if len(soft_deleted_info_df.head(1))==0:\r\n",
        "        soft_deleted_info_df = spark.createDataFrame([Row(\"9999-12-31T00:00:00Z\",0)],[\"ReportGenerationDate\",\"Size\"])\r\n",
        "    soft_deleted_info_df.write.mode('ignore').saveAsTable(\"{0}.SoftDeletedInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_content_type_distribution_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing File type and its corresponding size\r\n",
        "    if \"hdi_isfolder\" in csvFile.columns:\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['hdi_isfolder']==True,'Unknown').otherwise(substring_index(csvFile['Name'], '.', -1)))\r\n",
        "    else:\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['Name'].contains(\".\"),substring_index(csvFile['Name'], '.', -1)).otherwise('Unknown'))\r\n",
        "    content_type_info_by_count_df = content_type_info_df.groupBy(['ReportGenerationDate','FileType']).agg(sum('Content-Length').alias('Sum'))\r\n",
        "    content_type_info_by_count_df.write.mode('ignore').saveAsTable(\"{0}.ContentTypeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_growth_in_account_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing Date and corresponding data size on that day \r\n",
        "    growth_in_data_df = csvFile.groupBy('ReportGenerationDate').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\").orderBy(\"ReportGenerationDate\",\"Size\")\r\n",
        "    growth_in_data_df.write.mode('ignore').saveAsTable(\"{0}.dailyDataSizeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_creation_in_account_details(csvFile, database_name):\r\n",
        "    data_creation_with_time_df = csvFile.withColumn(\"Date\", to_date(csvFile['Creation-Time'], 'dd-MM-yyyy'))\r\n",
        "    data_creation_with_time_df = data_creation_with_time_df.groupBy('Date').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    data_creation_with_time_df.write.mode('ignore').saveAsTable(\"{0}.dataCreationWithTime\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_last_access_time_details(csvFile, database_name):\r\n",
        "    last_access_time_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\")\r\n",
        "    last_access_time_df = last_access_time_df.filter(last_access_time_df.AccessTier==\"Hot\")\r\n",
        "    if \"LastAccessTime\" in csvFile.columns and len(last_access_time_df.head(1))!=0:\r\n",
        "        max_ts = last_access_time_df.agg({\"ReportGenerationDate\": \"max\"}).collect()[0][0]\r\n",
        "        min_ts = last_access_time_df.agg({\"ReportGenerationDate\": \"min\"}).collect()[0][0]\r\n",
        "        last_access_time_df = last_access_time_df.filter(last_access_time_df.ReportGenerationDate==max_ts)\r\n",
        "        last_access_time_df = last_access_time_df.withColumn(\"DaysLastAccessed\", when((last_access_time_df[\"LastAccessTime\"].isNull() | (last_access_time_df[\"LastAccessTime\"]=='')),(datetime.datetime.now().date()-min_ts).days).otherwise(((unix_timestamp(current_date(),\"dd\") - unix_timestamp(last_access_time_df[\"LastAccessTime\"], \"dd\"))/86400).cast(IntegerType())))\r\n",
        "        last_access_time_df = last_access_time_df.withColumn(\"TotalSize\",lit(last_access_time_df.agg({\"Content-Length\":\"sum\"}).collect()[0][0]))\r\n",
        "        last_access_time_df.select(\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\").write.mode('ignore').saveAsTable(\"{0}.lastAccessTime\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_data_occupied_by_snapshot_details(csvFile, database_name):\r\n",
        "    if \"Snapshot\" in csvFile.columns:\r\n",
        "        snapshot_data_df = csvFile.na.drop(how='all',subset=['Snapshot'])\r\n",
        "        snapshot_data_df = snapshot_data_df.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "        snapshot_data_df.write.mode('ignore').saveAsTable(\"{0}.snapshotData\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_modifications_in_data_details(csvFile, database_name):\r\n",
        "    last_modified_count_df = csvFile.withColumn(\"Date\", to_date(csvFile['Last-Modified'], 'dd-MM-yyyy'))\r\n",
        "    last_modified_count_df = last_modified_count_df.groupBy('Date').agg(count('Date').alias('NumberOfModifications'))\r\n",
        "    last_modified_count_df.write.mode('ignore').saveAsTable(\"{0}.lastModifiedCount\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def store_reports_analysed_dates(csvFile, database_name):\r\n",
        "    reports_analysed_df = csvFile.select(\"ReportGenerationDate\").distinct()\r\n",
        "    reports_analysed_df.write.mode('ignore').saveAsTable(\"{0}.reportsanalysed\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name):\r\n",
        "    blob_service = BlockBlobService(storage_account_name, access_key)\r\n",
        "    blob_list = blob_service.list_blobs(destination_container)\r\n",
        "    links_list = []\r\n",
        "    for blob in blob_list:\r\n",
        "        if rule_name+\"-manifest.json\" in blob.name:\r\n",
        "            link = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container,storage_account_name,blob.name)\r\n",
        "            links_list.append(link)\r\n",
        "    return links_list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def populating_tables(csvFile, database_name):\r\n",
        "    store_data_distribution_in_containers_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_blob_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_access_tier_details(csvFile, database_name)\r\n",
        "    store_soft_deleted_data_size_details(csvFile, database_name)\r\n",
        "    store_content_type_distribution_details(csvFile, database_name)\r\n",
        "    store_data_growth_in_account_details(csvFile, database_name)\r\n",
        "    store_data_creation_in_account_details(csvFile, database_name)\r\n",
        "    store_last_access_time_details(csvFile, database_name)\r\n",
        "    store_data_occupied_by_snapshot_details(csvFile, database_name)\r\n",
        "    store_modifications_in_data_details(csvFile, database_name)\r\n",
        "    store_reports_analysed_dates(csvFile, database_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setting_credentials(storage_account_name, access_key):\r\n",
        "    access_link = \"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name)\r\n",
        "    spark.conf.set(access_link,access_key)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def processing_reports(list_of_report_links_json_file, report_dates_analysed):\r\n",
        "    report_df = spark.read.option(\"multiLine\", \"true\").json(list_of_report_links_json_file[0])\r\n",
        "    reports_analysed = []\r\n",
        "    data_collect = report_df.collect()[0]\r\n",
        "\r\n",
        "    # initialising dataframe with first row\r\n",
        "    destination_container = data_collect[\"destinationContainer\"]\r\n",
        "    prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "    report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "    reports_analysed.append(report_generation_date)\r\n",
        "    \r\n",
        "    file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "    fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "\r\n",
        "    if file_format==\"csv\":\r\n",
        "        fileData = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "    else:\r\n",
        "        fileData = spark.read.parquet(fileLink)\r\n",
        "    \r\n",
        "    # appending inventory report generation date to the dataframe\r\n",
        "    fileData = fileData.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "\r\n",
        "    # iterating over rest of the the rows\r\n",
        "    for json_file_report_link in list_of_report_links_json_file[1:]:\r\n",
        "\r\n",
        "        report_df = spark.read.option(\"multiLine\", \"true\").json(json_file_report_link)\r\n",
        "        data_collect = report_df.collect()[0]\r\n",
        "\r\n",
        "        if(data_collect[\"status\"]=='Pending'):\r\n",
        "            continue\r\n",
        "        \r\n",
        "        destination_container = data_collect[\"destinationContainer\"]\r\n",
        "        prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "        report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "\r\n",
        "        if report_generation_date in reports_analysed or report_generation_date in report_dates_analysed:\r\n",
        "            continue\r\n",
        "\r\n",
        "        file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "        fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "\r\n",
        "        if file_format==\"csv\":\r\n",
        "            fileDataTemp = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "        else:\r\n",
        "            fileDataTemp = spark.read.parquet(fileLink)\r\n",
        "\r\n",
        "        fileDataTemp = fileDataTemp.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "        reports_analysed.append(report_generation_date)\r\n",
        "        fileData = fileData.unionByName(fileDataTemp, allowMissingColumns=True)\r\n",
        "\r\n",
        "    # changing datatype of column Content-Length from string to integer\r\n",
        "    fileData = fileData.withColumn(\"Content-Length\", fileData[\"Content-Length\"].cast(IntegerType()))\r\n",
        "    # changing datatype of column ReportGenerationDate to Date type\r\n",
        "    fileData = fileData.withColumn(\"ReportGenerationDate\", to_date('ReportGenerationDate'))\r\n",
        "    # changing datatype of column LastAccessTime to Date type\r\n",
        "    fileData = fileData.withColumn(\"LastAccessTime\", to_date('LastAccessTime'))\r\n",
        "    return fileData"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_template_data(storage_account, container_name, file_name):\r\n",
        "    json_file_link = \"abfss://{0}@{1}.dfs.core.windows.net/{2}.json\".format(container_name, storage_account, file_name)\r\n",
        "    json_file = spark.read.option(\"multiLine\", \"true\").json(json_file_link)\r\n",
        "    json_file_data = json_file.collect()[0]\r\n",
        "    return json_file_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_report_dates(database_name):\r\n",
        "    # creating database if it does not exist\r\n",
        "    spark.sql(\"CREATE DATABASE IF NOT EXISTS {0}\".format(database_name))\r\n",
        "    table_list=spark.sql(\"SHOW TABLES IN {0}\".format(database_name))\r\n",
        "    table_data=table_list.filter(table_list.tableName==\"reportsanalysed\").collect()\r\n",
        "    table_res_list = []\r\n",
        "    if len(table_data)>0:\r\n",
        "        fetch_table = \"SELECT * FROM {0}.reportsanalysed\".format(database_name)\r\n",
        "        table_res = spark.sql(fetch_table)\r\n",
        "        # changing datatype of column ReportGenerationDate from DateType to StringType\r\n",
        "        table_res = table_res.withColumn(\"ReportGenerationDate\", table_res[\"ReportGenerationDate\"].cast(StringType()))\r\n",
        "        table_res_list = table_res.select(\"ReportGenerationDate\").rdd.flatMap(lambda x: x).collect()\r\n",
        "    return table_res_list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data from template file\r\n",
        "json_file_data = get_template_data(storage_account, container_name, file_name)\r\n",
        "\r\n",
        "# initializing with the data stored in template file\r\n",
        "storage_account_name = json_file_data[\"storageAccountName\"]\r\n",
        "access_key = json_file_data[\"accessKey\"]\r\n",
        "destination_container = json_file_data[\"destinationContainer\"]\r\n",
        "rule_name = json_file_data[\"ruleName\"]\r\n",
        "\r\n",
        "# setting credentials for the spark session\r\n",
        "setting_credentials(storage_account_name, access_key)\r\n",
        "\r\n",
        "# list of all the report links\r\n",
        "list_of_report_links = get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name)\r\n",
        "# print(\"No of reports - \",len(list_of_report_links))\r\n",
        "\r\n",
        "report_dates_analysed = get_report_dates(database_name)\r\n",
        "\r\n",
        "# processsing all the reports in a dataframe\r\n",
        "file_data = processing_reports(list_of_report_links, report_dates_analysed)\r\n",
        "\r\n",
        "# storing all processed dataframes in respective tables\r\n",
        "populating_tables(file_data, database_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# command to drop the database and the corresponding tables\r\n",
        "# database_name = \"temp1\"\r\n",
        "# spark.sql(\"DROP DATABASE IF EXISTS {0} CASCADE\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}