{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Blob Inventory Analytics Tool**"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the details of account and container where BlobInventoryStorageAccountConfiguration.json file is stored and the storage account \r\n",
        "# is connected to synapse workspace\r\n",
        "storage_account = \"reportanalysis\"\r\n",
        "container_name = \"reportdata\"\r\n",
        "file_name = \"BlobInventoryStorageAccountConfiguration\"\r\n",
        "\r\n",
        "# name of the database in which tables will be stored\r\n",
        "database_name = \"reportdata\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uninstalling current version as BlobService is disabled in current version\r\n",
        "!pip uninstall azure-storage-blob --yes\r\n",
        "# installing previous version as BlobService is available in this version\r\n",
        "!pip install azure-storage-blob==2.1.0"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\r\n",
        "from pyspark.sql import *\r\n",
        "from pyspark.sql.types import *\r\n",
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.functions import count as count\r\n",
        "from pyspark.sql.functions import substring_index\r\n",
        "from pyspark.sql.functions import lit\r\n",
        "from pyspark.sql.functions import col\r\n",
        "from azure.storage.blob import BlockBlobService\r\n",
        "import datetime"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing distribution of blobs in different containers \r\n",
        "def store_data_distribution_in_containers_details(csvFile, database_name):\r\n",
        "    # extracting container name from the 'Name' field\r\n",
        "    container_info_df = csvFile.withColumn(\"ContainerName\", substring_index(csvFile.Name, '/', 1))\r\n",
        "    # grouping dataframe on the basis of report generation date and then on the basis of container name\r\n",
        "    container_info_df = container_info_df.groupBy(['ReportGenerationDate','ContainerName']).sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    container_info_df.write.mode('ignore').saveAsTable(\"{0}.ContainerInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing table containing blobtype and their corresponding count with respect to every inventory report analysed ( i.e report generation date)\r\n",
        "def store_data_distribution_in_blob_details(csvFile, database_name):\r\n",
        "    # grouping dataframe on the basis of report generation date and then further on the basis of Blob Type\r\n",
        "    blob_type_info_df = csvFile.groupBy(['ReportGenerationDate','BlobType']).agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    blob_type_info_df.write.mode('ignore').saveAsTable(\"{0}.BlobTypeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the distribution of blobs in different access tiers\r\n",
        "def store_data_distribution_in_access_tier_details(csvFile, database_name):\r\n",
        "    # filtering dataframe that are block blob and then grouping on the basis of report generation date and further on the basis of access tier to which that blob belong\r\n",
        "    access_tier_info_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\").groupBy(['ReportGenerationDate','AccessTier']).agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    access_tier_info_df.write.mode('ignore').saveAsTable(\"{0}.AccessTierInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing soft deleted data size corresponding to the inventory report analysed\r\n",
        "def store_soft_deleted_data_size_details(csvFile, database_name):\r\n",
        "    if \"Deleted\" in csvFile.columns:\r\n",
        "        # filtering dataframe where Deleted column is set to True and then aggregating the Content-Length for those rows\r\n",
        "        soft_deleted_info_df = csvFile.filter(csvFile.Deleted==True)\r\n",
        "        # if there are is no soft deleted data, will fill the table with default value i.e 0 size\r\n",
        "        if soft_deleted_info_df.rdd.isEmpty():\r\n",
        "            soft_deleted_info_df = csvFile.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "            soft_deleted_info_df = soft_deleted_info_df.withColumn(\"Size\",lit(0))\r\n",
        "        else:\r\n",
        "            soft_deleted_info_df = soft_deleted_info_df.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    else:\r\n",
        "        # filling the table wih default values ( i.e Size is 0 ) if 'Deleted' Column is not present\r\n",
        "        soft_deleted_info_df = csvFile.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "        soft_deleted_info_df = soft_deleted_info_df.withColumn(\"Size\",lit(0))\r\n",
        "    soft_deleted_info_df.write.mode('ignore').saveAsTable(\"{0}.SoftDeletedInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the distribution of blob corresponding to the type of file ( ex - pdf, json, png )\r\n",
        "def store_content_type_distribution_details(csvFile, database_name):\r\n",
        "    # handling folder as a separate type of file\r\n",
        "    # filtering the dataframe for selecting folders by checking 'hdi_isfolder' column\r\n",
        "    if \"hdi_isfolder\" in csvFile.columns:\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['hdi_isfolder']==True,'Unknown').otherwise(substring_index(csvFile['Name'], '.', -1)))\r\n",
        "    else:\r\n",
        "        # selecting the right side of the last '.' in the Name column as the content-type of blob\r\n",
        "        content_type_info_df = csvFile.withColumn(\"FileType\", when(csvFile['Name'].contains(\".\"),substring_index(csvFile['Name'], '.', -1)).otherwise('Unknown'))\r\n",
        "    # grouping the dataframe on the basis of report generation date and further on the basis of type of file\r\n",
        "    content_type_info_by_count_df = content_type_info_df.groupBy(['ReportGenerationDate','FileType']).agg(sum('Content-Length').alias('Sum'))\r\n",
        "    content_type_info_by_count_df.write.mode('ignore').saveAsTable(\"{0}.ContentTypeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing Content-Length of the data corresponding to the report being analysed\r\n",
        "# it tells the growth in the size of data in the storage account\r\n",
        "def store_data_growth_in_account_details(csvFile, database_name):\r\n",
        "    # creating dataframe and storing table containing Date and corresponding data size on that day \r\n",
        "    # by grouping the data on the basis of report generation date and then aggregating the Content-Length\r\n",
        "    growth_in_data_df = csvFile.groupBy('ReportGenerationDate').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\").orderBy(\"ReportGenerationDate\",\"Size\")\r\n",
        "    growth_in_data_df.write.mode('ignore').saveAsTable(\"{0}.dailyDataSizeInfo\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the amount of data created on respective date\r\n",
        "def store_data_creation_in_account_details(csvFile, database_name):\r\n",
        "    # converting the 'Creation-Time' column to date type\r\n",
        "    data_creation_with_time_df = csvFile.withColumn(\"Date\", to_date(csvFile['Creation-Time'], 'dd-MM-yyyy'))\r\n",
        "    # grouping the dataframe on the basis of 'Creation-Date' and aggregating the Content-Length\r\n",
        "    data_creation_with_time_df = data_creation_with_time_df.groupBy('Date').sum('Content-Length').withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    data_creation_with_time_df.write.mode('ignore').saveAsTable(\"{0}.dataCreationWithTime\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the Name, Content-Length of the file along with the days for which it is not accessed (i.e Current Date minus the LastAccessTime)\r\n",
        "def store_last_access_time_details(csvFile, database_name):\r\n",
        "    \r\n",
        "    # filtering the dataframe whether the data is in Block blob\r\n",
        "    last_access_time_df = csvFile.filter(csvFile.BlobType==\"BlockBlob\")\r\n",
        "    # storing the date of the last inventory report analysed\r\n",
        "    max_ts = csvFile.agg({\"ReportGenerationDate\": \"max\"}).collect()[0][0]\r\n",
        "    # storing the date to the first inventory report analysed\r\n",
        "    min_ts = csvFile.agg({\"ReportGenerationDate\": \"min\"}).collect()[0][0]\r\n",
        "\r\n",
        "    last_access_time_hot_tier_df = last_access_time_df.filter(last_access_time_df.AccessTier==\"Hot\")\r\n",
        "    last_access_time_cool_tier_df = last_access_time_df.filter(last_access_time_df.AccessTier==\"Cool\")\r\n",
        "  \r\n",
        "    # filtering the dataframe on the basis of the last report analysed (date basis) as we will only use last report to get the LastAccessTime\r\n",
        "    last_access_time_hot_tier_df = last_access_time_hot_tier_df.filter(last_access_time_hot_tier_df.ReportGenerationDate==max_ts)\r\n",
        "    if \"LastAccessTime\" in csvFile.columns:\r\n",
        "        # changing datatype of column LastAccessTime to Date type\r\n",
        "        last_access_time_hot_tier_df = last_access_time_hot_tier_df.withColumn(\"LastAccessTime\", to_date('LastAccessTime'))\r\n",
        "        # if LastAccessTime is null or empty, will fill it with the total span of days for which the reports are generated calcuated by subtracting current date minus the date of first report generated(current date  - min_ts)\r\n",
        "        last_access_time_hot_tier_df = last_access_time_hot_tier_df.withColumn(\"DaysLastAccessed\", when((last_access_time_hot_tier_df[\"LastAccessTime\"].isNull() | (last_access_time_hot_tier_df[\"LastAccessTime\"]=='')),(datetime.datetime.now().date()-min_ts).days).otherwise(((unix_timestamp(current_date(),\"dd\") - unix_timestamp(last_access_time_hot_tier_df[\"LastAccessTime\"], \"dd\"))/86400).cast(IntegerType())))\r\n",
        "    else:\r\n",
        "        # using default value for 'DaysLastAccessed' as the number of days between current date and first generated report date\r\n",
        "        last_access_time_hot_tier_df = last_access_time_hot_tier_df.withColumn(\"DaysLastAccessed\", (datetime.datetime.now().date()-min_ts).days)\r\n",
        "    # creating a column 'TotalSize' that stores the total amount of data present in hot tier and is block blob\r\n",
        "    last_access_time_hot_tier_df = last_access_time_hot_tier_df.withColumn(\"TotalSize\",lit(last_access_time_hot_tier_df.agg({\"Content-Length\":\"sum\"}).collect()[0][0]))\r\n",
        "    # if the dataframe is empty, then will fill with default values\r\n",
        "    if last_access_time_hot_tier_df.rdd.isEmpty():\r\n",
        "        last_access_time_hot_tier_df = spark.createDataFrame([Row(max_ts,\"Name1\",0,0,0),Row(min_ts,\"Name2\",0,(datetime.datetime.now().date()-min_ts).days,0)],[\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\"])\r\n",
        "    last_access_time_hot_tier_df.select(\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\").write.mode('ignore').saveAsTable(\"{0}.lastAccessTimeHotTier\".format(database_name))\r\n",
        "    \r\n",
        "\r\n",
        "    # processing dataframe for cool tier\r\n",
        "    # filtering the dataframe on the basis of the last report analysed (date basis) as we will only use last report to get the LastAccessTime\r\n",
        "    last_access_time_cool_tier_df = last_access_time_cool_tier_df.filter(last_access_time_cool_tier_df.ReportGenerationDate==max_ts)\r\n",
        "    if \"LastAccessTime\" in csvFile.columns:\r\n",
        "        # changing datatype of column LastAccessTime to Date type\r\n",
        "        last_access_time_cool_tier_df = last_access_time_cool_tier_df.withColumn(\"LastAccessTime\", to_date('LastAccessTime'))\r\n",
        "        # if LastAccessTime is null or empty, will fill it with the totl span of days for which the reports are generated calcuated by subtracting current date minus the date of first report generated(current date  - min_ts)\r\n",
        "        last_access_time_cool_tier_df = last_access_time_cool_tier_df.withColumn(\"DaysLastAccessed\", when((last_access_time_cool_tier_df[\"LastAccessTime\"].isNull() | (last_access_time_cool_tier_df[\"LastAccessTime\"]=='')),(datetime.datetime.now().date()-min_ts).days).otherwise(((unix_timestamp(current_date(),\"dd\") - unix_timestamp(last_access_time_cool_tier_df[\"LastAccessTime\"], \"dd\"))/86400).cast(IntegerType())))\r\n",
        "    else:\r\n",
        "        # using default value for 'DaysLastAccessed' as the number of days between current date and first generated report date\r\n",
        "        last_access_time_cool_tier_df = last_access_time_cool_tier_df.withColumn(\"DaysLastAccessed\", (datetime.datetime.now().date()-min_ts).days)\r\n",
        "    # creating a column 'TotalSize' that stores the total amount of data present in hot tier and is block blob\r\n",
        "    last_access_time_cool_tier_df = last_access_time_cool_tier_df.withColumn(\"TotalSize\",lit(last_access_time_cool_tier_df.agg({\"Content-Length\":\"sum\"}).collect()[0][0]))\r\n",
        "    # if the dataframe is empty, then will fill with default values\r\n",
        "    if last_access_time_cool_tier_df.rdd.isEmpty():\r\n",
        "        last_access_time_cool_tier_df = spark.createDataFrame([Row(max_ts,\"Name1\",0,0,0),Row(min_ts,\"Name2\",0,(datetime.datetime.now().date()-min_ts).days,0)],[\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\"])\r\n",
        "    last_access_time_cool_tier_df.select(\"ReportGenerationDate\",\"Name\",\"Content-Length\",\"DaysLastAccessed\",\"TotalSize\").write.mode('ignore').saveAsTable(\"{0}.lastAccessTimeCoolTier\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the size of data occupied by snapshots\r\n",
        "def store_data_occupied_by_snapshot_details(csvFile, database_name):\r\n",
        "    # checking whether 'Snapshot' field is present in the inventory report(dataframe)\r\n",
        "    if \"Snapshot\" in csvFile.columns:\r\n",
        "        # dropping all the rows having null values in Snapshot column\r\n",
        "        snapshot_data_df = csvFile.na.drop(how='all',subset=['Snapshot'])\r\n",
        "        if snapshot_data_df.rdd.isEmpty():\r\n",
        "            snapshot_data_df = csvFile.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "            # using default value of Snapshot size as 0 as 'Snapshot' field is disabled\r\n",
        "            snapshot_data_df = snapshot_data_df.withColumn(\"Size\", lit(0))\r\n",
        "        else:\r\n",
        "            # grouping dataframe on the basis of report generation date and then aggregating the Content-Length\r\n",
        "            snapshot_data_df = snapshot_data_df.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "    else:\r\n",
        "        snapshot_data_df = csvFile.groupBy(\"ReportGenerationDate\").agg({'Content-Length':'sum'}).withColumnRenamed(\"sum(Content-Length)\", \"Size\")\r\n",
        "        # using default value of Snapshot size as 0 as 'Snapshot' field is disabled\r\n",
        "        snapshot_data_df = snapshot_data_df.withColumn(\"Size\", lit(0))\r\n",
        "    snapshot_data_df.write.mode('ignore').saveAsTable(\"{0}.snapshotData\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing number of modifications done corresponding to the respective date\r\n",
        "def store_modifications_in_data_details(csvFile, database_name):\r\n",
        "    #checking whether Last-Modified field is present in the inventory report\r\n",
        "    if \"Last-Modified\" in csvFile.columns:\r\n",
        "        last_modified_count_df = csvFile.withColumn(\"Date\", to_date(csvFile['Last-Modified'], 'dd-MM-yyyy'))\r\n",
        "        last_modified_count_df = last_modified_count_df.groupBy('Date').agg(count('Date').alias('NumberOfModifications'))\r\n",
        "    else:\r\n",
        "        last_modified_count_df = csvFile.groupBy('ReportGenerationDate').agg(count('ReportGenerationDate').alias('NumberOfModifications'))\r\n",
        "        # using default value as 0 to show that 'Last-Modified' field is not present and we need entry for every report date analysed otherwise it will hamper the relationship between tables\r\n",
        "        last_modified_count_df = last_modified_count_df.withColumn(\"NumberOfModifications\", lit(0))\r\n",
        "    last_modified_count_df.write.mode('ignore').saveAsTable(\"{0}.lastModifiedCount\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the dates i.e report generation date of all the reports analysed\r\n",
        "def store_reports_analysed_dates(csvFile, database_name):\r\n",
        "    reports_analysed_df = csvFile.select(\"ReportGenerationDate\").distinct()\r\n",
        "    reports_analysed_df.write.mode('ignore').saveAsTable(\"{0}.reportsanalysed\".format(database_name))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns all the json file links to the respective inventory reports (by providing the destination container and the rule name along with storage account credentials)\r\n",
        "def get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name):\r\n",
        "    blob_service = BlockBlobService(storage_account_name, access_key)\r\n",
        "    # list the relative path to all the blobs present in the destination container\r\n",
        "    try:\r\n",
        "        blob_list = blob_service.list_blobs(destination_container)\r\n",
        "    except:\r\n",
        "        print(\"Error: Container does not exist\")\r\n",
        "        return\r\n",
        "    # storing the links to all the blob inventory reports\r\n",
        "    links_list = []\r\n",
        "    # iterating over the returned list of relative path to blobs\r\n",
        "    for blob in blob_list:\r\n",
        "        # checking if the relative path contains 'ruleName-manifest.json' and correspondingly creating a link to that json file\r\n",
        "        if rule_name+\"-manifest.json\" in blob.name:\r\n",
        "            link = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container,storage_account_name,blob.name)\r\n",
        "            links_list.append(link)\r\n",
        "    return links_list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calling functions for respective processing and storing them in the form of tables\r\n",
        "# every table corresponds to respective visualization\r\n",
        "def populating_tables(csvFile, database_name):\r\n",
        "    # checking if the dataframe is empty that is no data to process\r\n",
        "    if csvFile.rdd.isEmpty():\r\n",
        "        return\r\n",
        "    store_data_distribution_in_containers_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_blob_details(csvFile, database_name)\r\n",
        "    store_data_distribution_in_access_tier_details(csvFile, database_name)\r\n",
        "    store_soft_deleted_data_size_details(csvFile, database_name)\r\n",
        "    store_content_type_distribution_details(csvFile, database_name)\r\n",
        "    store_data_growth_in_account_details(csvFile, database_name)\r\n",
        "    store_data_creation_in_account_details(csvFile, database_name)\r\n",
        "    store_last_access_time_details(csvFile, database_name)\r\n",
        "    store_data_occupied_by_snapshot_details(csvFile, database_name)\r\n",
        "    store_modifications_in_data_details(csvFile, database_name)\r\n",
        "    store_reports_analysed_dates(csvFile, database_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# setting the credentials of the spark session\r\n",
        "def setting_credentials(storage_account_name, access_key):\r\n",
        "    access_link = \"fs.azure.account.key.{0}.blob.core.windows.net\".format(storage_account_name)\r\n",
        "    try:\r\n",
        "        spark.conf.set(access_link,access_key)\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to set credentials for spark session\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# processing multiple inventory reports by processing their respective json file and adding them to the dataframe\r\n",
        "def processing_reports(list_of_report_links_json_file, report_dates_analysed):\r\n",
        "\r\n",
        "    try:\r\n",
        "        report_df = spark.read.option(\"multiLine\", \"true\").json(list_of_report_links_json_file[0])\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to access Blob Inventory Report Json File\")\r\n",
        "        return\r\n",
        "\r\n",
        "    try:\r\n",
        "        data_collect = report_df.collect()[0]\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to read Blob Inventory Report Json File\")\r\n",
        "        return\r\n",
        "\r\n",
        "    # initialising dataframe with first row\r\n",
        "    try:\r\n",
        "        destination_container = data_collect[\"destinationContainer\"]\r\n",
        "        prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "        report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "        file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "    except:\r\n",
        "        print(\"Error: Missing Fields in Inventory Report Json File\")\r\n",
        "        return\r\n",
        "    \r\n",
        "    reports_analysed = []\r\n",
        "    # appending the report analysed generation date to the list\r\n",
        "    reports_analysed.append(report_generation_date)\r\n",
        "    # creating a link to the inventory report\r\n",
        "    fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "\r\n",
        "    # checking whether the report is in csv or parquet format\r\n",
        "    try:\r\n",
        "        if file_format==\"csv\":\r\n",
        "            fileData = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "        else:\r\n",
        "            fileData = spark.read.parquet(fileLink)\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to access Blob Inventory Reports\")\r\n",
        "        return\r\n",
        "    \r\n",
        "    # appending inventory report generation date to the dataframe\r\n",
        "    fileData = fileData.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "\r\n",
        "    # iterating over rest of the the rows\r\n",
        "    for json_file_report_link in list_of_report_links_json_file[1:]:\r\n",
        "        try:\r\n",
        "            report_df = spark.read.option(\"multiLine\", \"true\").json(json_file_report_link)\r\n",
        "        except:\r\n",
        "            print(\"Error: Unable to access Blob Inventory Report Json File\")\r\n",
        "            return\r\n",
        "\r\n",
        "        try:\r\n",
        "            data_collect = report_df.collect()[0]\r\n",
        "        except:\r\n",
        "            print(\"Error: Unable to read Blob Inventory Report Json File\")\r\n",
        "            return\r\n",
        "\r\n",
        "        # skip if the report is pending\r\n",
        "        if(data_collect[\"status\"]=='Pending'):\r\n",
        "            continue\r\n",
        "\r\n",
        "        destination_container = data_collect[\"destinationContainer\"]\r\n",
        "        prefix_path = data_collect[\"files\"][0][\"blob\"]\r\n",
        "        report_generation_date = data_collect[\"inventoryCompletionTime\"].split(\"T\")[0]\r\n",
        "        file_format = data_collect[\"ruleDefinition\"][\"format\"]\r\n",
        "      \r\n",
        "        # skip if the report if already analysed\r\n",
        "        if report_generation_date in reports_analysed or report_generation_date in report_dates_analysed:\r\n",
        "            continue\r\n",
        "\r\n",
        "        fileLink = \"wasbs://{0}@{1}.blob.core.windows.net/{2}\".format(destination_container, storage_account_name, prefix_path)\r\n",
        "\r\n",
        "        # checking whether the report is in csv or parquet format\r\n",
        "        try:\r\n",
        "            if file_format==\"csv\":\r\n",
        "                fileDataTemp = spark.read.csv(fileLink, header=True, inferSchema=True)\r\n",
        "            else:\r\n",
        "                fileDataTemp = spark.read.parquet(fileLink)\r\n",
        "        except:\r\n",
        "            print(\"Error: Unable to access Blob Inventory Reports\")\r\n",
        "            return\r\n",
        "\r\n",
        "        fileDataTemp = fileDataTemp.withColumn('ReportGenerationDate',lit(report_generation_date))\r\n",
        "        reports_analysed.append(report_generation_date)\r\n",
        "        try:\r\n",
        "            fileData = fileData.unionByName(fileDataTemp, allowMissingColumns=True)\r\n",
        "        except:\r\n",
        "            print(\"Error: Blob Inventory Reports have mis-matching fields\")\r\n",
        "            return\r\n",
        "\r\n",
        "    # changing datatype of column Content-Length from string to integer\r\n",
        "    try:\r\n",
        "        fileData = fileData.withColumn(\"Content-Length\", fileData[\"Content-Length\"].cast(IntegerType()))\r\n",
        "    except:\r\n",
        "        print(\"Error: Content-Length field is missing from the inventory reports\")\r\n",
        "    # changing datatype of column ReportGenerationDate to Date type\r\n",
        "    fileData = fileData.withColumn(\"ReportGenerationDate\", to_date('ReportGenerationDate'))\r\n",
        "    \r\n",
        "    return fileData"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# processing and returning the configuration file data\r\n",
        "def get_configuration_file_data(storage_account, container_name, file_name):\r\n",
        "    configuration_file_link = \"abfss://{0}@{1}.dfs.core.windows.net/{2}.json\".format(container_name, storage_account, file_name)\r\n",
        "    try:\r\n",
        "        configuration_file = spark.read.option(\"multiLine\", \"true\").json(configuration_file_link)\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to access Blob Inventory Storage Account Configuration File\")\r\n",
        "        return\r\n",
        "    try:\r\n",
        "        configuration_file_data = configuration_file.collect()[0]\r\n",
        "    except:\r\n",
        "        print(\"Error: Unable to read Blob Inventory Storage Account Configuration File\")\r\n",
        "        return\r\n",
        "    return configuration_file_data"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# returns the list of all the reports that are analysed till now by returning the data stored in the table 'reportsanalysed'\r\n",
        "def get_report_dates(database_name):\r\n",
        "    # creating database if it does not exist\r\n",
        "    spark.sql(\"CREATE DATABASE IF NOT EXISTS {0}\".format(database_name))\r\n",
        "    # retrieving al the tables stored in the database\r\n",
        "    table_list=spark.sql(\"SHOW TABLES IN {0}\".format(database_name))\r\n",
        "    # filtering whether 'reportsanalysed' table is present in the database \r\n",
        "    table_data=table_list.filter(table_list.tableName==\"reportsanalysed\").collect()\r\n",
        "    # list of all the reports analysed \r\n",
        "    table_res_list = []\r\n",
        "    if len(table_data)>0:\r\n",
        "        # fetching the 'reportsanalysed' table\r\n",
        "        fetch_table = \"SELECT * FROM {0}.reportsanalysed\".format(database_name)\r\n",
        "        table_res = spark.sql(fetch_table)\r\n",
        "        # changing datatype of column ReportGenerationDate from DateType to StringType\r\n",
        "        table_res = table_res.withColumn(\"ReportGenerationDate\", table_res[\"ReportGenerationDate\"].cast(StringType()))\r\n",
        "        # converting the dataframe to the list type\r\n",
        "        table_res_list = table_res.select(\"ReportGenerationDate\").rdd.flatMap(lambda x: x).collect()\r\n",
        "    return table_res_list"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the data from template file\r\n",
        "json_file_data = get_configuration_file_data(storage_account, container_name, file_name)\r\n",
        "\r\n",
        "# initializing with the data stored in template file\r\n",
        "try:\r\n",
        "    storage_account_name = json_file_data[\"storageAccountName\"]\r\n",
        "    access_key = json_file_data[\"accessKey\"]\r\n",
        "    destination_container = json_file_data[\"destinationContainer\"]\r\n",
        "    rule_name = json_file_data[\"blobInventoryRuleName\"]\r\n",
        "except:\r\n",
        "    print(\"Error: Invalid Format of Blob Inventory Storage Account Configuration File\")\r\n",
        "\r\n",
        "# setting credentials for the spark session\r\n",
        "setting_credentials(storage_account_name, access_key)\r\n",
        "\r\n",
        "# list of all the report links\r\n",
        "list_of_report_links = get_json_link_of_reports(storage_account_name, access_key, destination_container, rule_name)\r\n",
        "\r\n",
        "if(len(list_of_report_links)!=0):\r\n",
        "    # retreving all the reports analysed till now\r\n",
        "    report_dates_analysed = get_report_dates(database_name)\r\n",
        "\r\n",
        "    # processsing all the reports in a dataframe\r\n",
        "    file_data = processing_reports(list_of_report_links, report_dates_analysed)\r\n",
        "\r\n",
        "    # storing all processed dataframes in respective tables\r\n",
        "    populating_tables(file_data, database_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# database_name = \"reportdata\"\r\n",
        "# support SQL queries\r\n",
        "\r\n",
        "# command to drop the database and the corresponding tables\r\n",
        "# spark.sql(\"DROP DATABASE IF EXISTS {0} CASCADE\".format(database_name))\r\n",
        "\r\n",
        "# command to iterate over a table\r\n",
        "# table_name = \"blobtypeinfo\"\r\n",
        "# spark.sql('SELECT * from {0}.{1}'.format(database_name, table_name)).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "language": "Python",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}